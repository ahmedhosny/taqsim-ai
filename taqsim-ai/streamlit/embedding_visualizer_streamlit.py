"""
Streamlit Embedding Visualizer for Taqsim AI

This module provides Streamlit components for visualizing audio embeddings
generated by the MAEST model. It integrates with the existing embedding
visualization pipeline but presents the results in a Streamlit interface.
"""

import glob
import os

import altair as alt
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from umap import UMAP

# Define base paths relative to the script location
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(SCRIPT_DIR)  # Go one level up
DATA_DIR = os.path.join(PARENT_DIR, "data")
METADATA_CSV_PATH = os.path.join(DATA_DIR, "taqsim_ai.csv")
EMBEDDINGS_DIR_PATH = os.path.join(DATA_DIR, "embeddings")
NORMALIZED_EMBEDDINGS_DIR_PATH = os.path.join(DATA_DIR, "normalized_embeddings")

# Global toggle for verbose info messages
SHOW_VERBOSE_INFO = False


def get_all_artists_from_csv():
    """
    Reads the metadata CSV and returns a sorted list of unique artist names.
    Handles potential missing 'artist' column or file errors.
    """
    csv_path = METADATA_CSV_PATH
    try:
        df = pd.read_csv(csv_path)
        if "artist" in df.columns:
            # Get unique artists, convert to string, strip whitespace, handle empty strings, sort
            # Fill NaN and empty strings with a consistent placeholder first
            # Then convert to string, strip, and replace placeholder and 'nan' string with 'Unknown Artist'
            # 1. Convert 'artist' column to string type.
            series = df["artist"].astype(str)
            # 2. Convert to lowercase.
            series = series.str.lower()
            # 3. Strip whitespace from all entries.
            series = series.str.strip()
            # 4. Replace empty strings and the literal string 'nan' (which is now guaranteed lowercase)
            #    with a canonical lowercase 'unknown artist'.
            series = series.replace(["", "nan"], "unknown artist")
            # 5. Get unique, sorted list of artist names.
            artists = sorted(list(series.unique()))
            # Ensure "Unknown Artist" is treated as one category if it results from multiple sources (NaN, empty, 'nan')
            # The .unique() above should handle this, but let's be sure it's clean.
            # If "Unknown Artist" is the only thing and it's not desired, could filter here.
            return artists
        else:
            # Log to sidebar or main page depending on context if Streamlit elements are used here
            # For a pure helper, returning empty and letting caller handle st messages is cleaner
            # st.sidebar.warning("Metadata CSV is missing 'artist' column.")
            return []
    except FileNotFoundError:
        # st.sidebar.error(f"Metadata CSV file not found at: {csv_path}")
        return []


# Helper function to get all unique song names from the metadata CSV
def get_all_songs_from_csv():
    csv_path = METADATA_CSV_PATH
    try:
        df = pd.read_csv(csv_path)
        if "song_name" in df.columns:
            songs = sorted(
                list(
                    df["song_name"]
                    .astype(str)
                    .str.strip()
                    .replace("", "Unknown Song")
                    .fillna(
                        "Unknown Song"
                    )  # Note: fillna after astype(str) might not catch all original NaNs if they became 'nan' string
                    .unique()
                )
            )
            return songs
        else:
            return []
    except FileNotFoundError:
        return []
    except Exception:
        # st.sidebar.error(f"Error reading songs from CSV: {e}") # Consider context for st messages
        return []


# Helper function to get all unique maqam names from the metadata CSV
def get_all_maqams_from_csv():
    csv_path = METADATA_CSV_PATH
    try:
        df = pd.read_csv(csv_path)
        if "maqam" in df.columns:
            maqams = sorted(
                list(
                    df["maqam"]
                    .astype(str)
                    .str.strip()
                    .replace("", "Unknown Maqam")
                    .fillna("Unknown Maqam")
                    .unique()
                )
            )
            return maqams
        else:
            return []
    except FileNotFoundError:
        return []
    except Exception:
        # st.sidebar.error(f"Error reading maqams from CSV: {e}") # Consider context for st messages
        return []
    except Exception as _e:
        # st.sidebar.error(f"Error reading artists from CSV: {_e}")
        return []


def load_embeddings(embeddings_dir):
    """
    Load embeddings from the specified directory.

    Args:
        embeddings_dir: Directory containing embedding files

    Returns:
        Dictionary mapping video IDs to embeddings
    """
    all_embeddings = {}
    failed_files = []  # Initialize list to store names of files that failed to load

    # Find all .npz files in the embeddings directory
    # The pattern is: {uuid}_{chunk_number}_{start_second}_{end_second}.npz
    embedding_files = glob.glob(os.path.join(embeddings_dir, "*.npz"))

    if not embedding_files:
        st.warning(f"No embedding files found in {embeddings_dir}")
        return all_embeddings

    if SHOW_VERBOSE_INFO:
        st.info(f"Found {len(embedding_files)} embedding files")

    for file_path in embedding_files:
        # Extract video ID and chunk number from filename
        basename = os.path.basename(file_path)
        base_name_no_ext = os.path.splitext(basename)[0]
        parts = base_name_no_ext.split("_")

        # Need at least 4 parts: uuid, chunk_number, start_second, end_second
        if len(parts) < 4:
            st.warning(f"Skipping file with invalid name format: {basename}")
            continue

        # The UUID is the first part
        video_id = parts[0]
        # The chunk number is the second part
        chunk_num = parts[1]
        chunk_key = f"chunk_{chunk_num}"

        # Load the embedding from the NPZ file
        try:
            embedding_data = np.load(file_path)
            # Check if the file contains the 'embedding' key
            if "embedding" in embedding_data:
                embedding = embedding_data["embedding"]
                # Initialize the dictionary for this video ID if it doesn't exist
                if video_id not in all_embeddings:
                    all_embeddings[video_id] = {}
                # Add the embedding for this chunk
                all_embeddings[video_id][chunk_key] = embedding
            else:
                st.warning(f"File {basename} does not contain 'embedding' key")
        except Exception as e:
            st.error(f"Error loading embedding from {basename}: {e}")
            failed_files.append(basename)

    # Print some statistics
    if SHOW_VERBOSE_INFO:
        st.info(
            f"Loaded {len(all_embeddings)} videos with embeddings. "
            f"{len(failed_files)} files failed to load."
        )

    return all_embeddings


def get_metadata_from_csv(video_ids):
    """
    Get metadata for the specified video IDs from the CSV file.

    Args:
        video_ids: List of video IDs to get metadata for

    Returns:
        Dictionary mapping video IDs to their metadata
    """
    metadata = {}

    csv_path = METADATA_CSV_PATH

    try:
        df = pd.read_csv(csv_path)

        if df.empty:
            st.warning("CSV file is empty")
            return metadata

        if "uuid" not in df.columns:
            st.warning("CSV file is missing 'uuid' column. Cannot match metadata.")
            return metadata

        # Count how many UUIDs match
        matched_count = 0

        for _, row in df.iterrows():
            try:
                if "uuid" in row:
                    uuid = str(row["uuid"])
                    if uuid in video_ids:
                        # Create a dictionary of metadata for this video
                        video_metadata = {}
                        for col in df.columns:
                            if col != "uuid":
                                video_metadata[col] = row[col]
                        metadata[uuid] = video_metadata
                        matched_count += 1
            except Exception as e:
                st.error(f"Error processing row in CSV: {e}")

        if SHOW_VERBOSE_INFO:
            st.info(f"Found metadata for {matched_count} out of {len(video_ids)} UUIDs")
    except Exception as e:
        st.error(f"Error reading CSV file: {e}")

    return metadata


def prepare_embeddings_for_umap(
    all_embeddings,
    embedding_type="cls",
    exclude_last_chunk=False,
    only_first_chunk=False,
    normalize_by_artist=False,
):
    """
    Prepare embeddings for UMAP dimensionality reduction.
    Can optionally exclude the last chunk from each video or only include the first chunk.

    Args:
        all_embeddings: Dictionary of embeddings by video ID and chunk
        embedding_type: Which embedding type to use ('cls', 'dist', 'avg', or 'combined')
        exclude_last_chunk: If True, exclude the last chunk from each video
        only_first_chunk: If True, only include the first chunk from each video
        normalize_by_artist: If True, apply additional artist normalization (not needed if using pre-normalized embeddings)

    Returns:
        Tuple of (embeddings array, video_ids list, chunk_numbers list)
    """
    embeddings_list = []
    video_ids_list = []
    chunk_numbers = []

    for video_id, video_embeddings in all_embeddings.items():
        # If we need to exclude the last chunk, find the maximum chunk number
        max_chunk = -1
        min_chunk = float("inf")
        if (exclude_last_chunk or only_first_chunk) and video_embeddings:
            chunk_numbers_list = [
                int(chunk_key.split("_")[1]) for chunk_key in video_embeddings.keys()
            ]
            if exclude_last_chunk:
                max_chunk = max(chunk_numbers_list)
            if only_first_chunk:
                min_chunk = min(chunk_numbers_list)

        for chunk_key, embedding in video_embeddings.items():
            # Extract chunk number from key (e.g., 'chunk_1' -> 1)
            chunk_num = int(chunk_key.split("_")[1])

            # Skip this chunk if it's the last one and we're excluding last chunks
            if exclude_last_chunk and chunk_num == max_chunk:
                continue

            # Skip this chunk if it's not the first one and we're only including first chunks
            if only_first_chunk and chunk_num != min_chunk:
                continue

            # Check the shape of the embedding to determine its structure
            if len(embedding.shape) == 2 and embedding.shape[0] == 3:
                # Original format with [3, 768] shape where:
                # embedding[0] = CLS token embedding
                # embedding[1] = DIST token embedding
                # embedding[2] = Average of other token embeddings
                if embedding_type == "cls":
                    embeddings_list.append(embedding[0])
                elif embedding_type == "dist":
                    embeddings_list.append(embedding[1])
                elif embedding_type == "avg":
                    embeddings_list.append(embedding[2])
                elif embedding_type == "combined":
                    # Concatenate all three embeddings
                    embeddings_list.append(
                        np.concatenate([embedding[0], embedding[1], embedding[2]])
                    )
            else:
                # New format where the embedding is already a single vector
                embeddings_list.append(embedding)

            video_ids_list.append(video_id)
            chunk_numbers.append(chunk_num)

    # Convert to numpy arrays for UMAP
    if embeddings_list:
        embeddings_array = np.vstack(embeddings_list)
        return embeddings_array, video_ids_list, chunk_numbers
    else:
        st.warning("No valid embeddings found for the selected embedding type.")
        return np.array([]), [], []


def create_embedding_visualization(
    embeddings_dir: str,
    embedding_type: str = "cls",
    reduction_method: str = "UMAP",  # Added with type hint and default
    exclude_last_chunk: bool = False,
    artist_filter: list[str] | None = None,
    song_filter: list[str] | None = None,
    maqam_filter: list[str] | None = None,
    color_selection: str = "artist",
    show_lines: bool = False,
    show_chunk_numbers: bool = False,
    only_first_chunk: bool = False,
    show_only_selected_artist_labels: bool = False,
    debug: bool = False,
    normalize_by_artist: bool = False,
    n_neighbors: int = 15,
):
    """
    Create an interactive visualization of audio embeddings using Altair.

    Args:
        embeddings_dir: Directory containing embedding files
        embedding_type: Type of embedding to use ('cls', 'dist', 'avg', or 'combined')
        reduction_method: Dimensionality reduction method ('UMAP', 'T-SNE', or 'PCA')
        exclude_last_chunk: Whether to exclude the last chunk from each video
        artist_filter: List of artists to include (None = all artists)
        song_filter: List of songs to include (None = all songs)
        maqam_filter: List of maqams to include (None = all maqams)
        color_selection: Field to use for coloring points ('artist', 'song_name', or 'maqam')
        show_lines: Whether to show lines connecting chunks from the same video
        show_chunk_numbers: Whether to show chunk numbers as text labels
        only_first_chunk: Whether to only include the first chunk from each video
        show_only_selected_artist_labels: Whether to only show labels for selected artists
        debug: Whether to print debug information
        normalize_by_artist: Whether to normalize embeddings by artist
        n_neighbors: Number of neighbors to consider for UMAP (only used when reduction_method is 'UMAP')

    Returns:
        Altair chart object or None if no embeddings found
    """

    # Load all embeddings
    all_embeddings = load_embeddings(embeddings_dir)
    if not all_embeddings:
        st.error("No embeddings found. Please check the embeddings directory.")
        return None

    # Prepare embeddings for UMAP
    embedding_array, video_ids, chunk_numbers = prepare_embeddings_for_umap(
        all_embeddings,
        embedding_type,
        exclude_last_chunk,
        only_first_chunk,
        normalize_by_artist,
    )

    # Check if we have any embeddings to visualize
    if len(embedding_array) == 0:
        st.error(
            f"No embeddings found for type '{embedding_type}'. Try a different embedding type."
        )
        return None

    # Get metadata for all video IDs
    unique_video_ids = list(set(video_ids))
    metadata_by_video = get_metadata_from_csv(unique_video_ids)

    # Reduce dimensionality based on selected method
    if reduction_method == "UMAP":
        with st.spinner(
            f"Reducing dimensionality with UMAP (embedding type: {embedding_type})..."
        ):
            reducer = UMAP(
                n_neighbors=n_neighbors, min_dist=0.1, random_state=42, n_components=2
            )
            embedding_2d = reducer.fit_transform(embedding_array)
    elif reduction_method == "T-SNE":
        with st.spinner(
            f"Reducing dimensionality with T-SNE (embedding type: {embedding_type})..."
        ):
            # T-SNE can be slow on large datasets, consider perplexity and n_iter
            # For very large datasets, PCA might be a prerequisite for T-SNE
            reducer = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)
            embedding_2d = reducer.fit_transform(embedding_array)
    elif reduction_method == "PCA":
        with st.spinner(
            f"Reducing dimensionality with PCA (embedding type: {embedding_type})..."
        ):
            reducer = PCA(n_components=2, random_state=42)
            embedding_2d = reducer.fit_transform(embedding_array)
    else:
        st.error(f"Unknown reduction method: {reduction_method}")
        return None

    # Create a DataFrame for Altair with all metadata
    df_data = {
        "x": embedding_2d[:, 0],
        "y": embedding_2d[:, 1],
        "video_id": video_ids,
        "chunk_number": chunk_numbers,
    }

    # Create a mapping from each video_id to its metadata
    # This ensures consistent metadata for all chunks from the same video
    for column in [
        "song_name",
        "artist",
        "maqam",
        "type",
        "electric",
        "vintage",
        "link",
    ]:
        column_values = []
        for vid in video_ids:
            # Get metadata for this video ID
            metadata = metadata_by_video.get(vid, {})
            # Use lowercase 'unknown artist' as the standard for missing/invalid artist names
            default_value = "unknown artist" if column == "artist" else "Unknown"
            raw_value = metadata.get(column, default_value)

            if column == "artist":
                # 1. Convert to string type.
                processed_value = str(raw_value)
                # 2. Convert to lowercase.
                processed_value = processed_value.lower()
                # 3. Strip whitespace.
                processed_value = processed_value.strip()
                # 4. Replace empty strings and the literal string 'nan' (now guaranteed lowercase)
                #    with the canonical lowercase 'unknown artist'.
                if processed_value == "" or processed_value == "nan":
                    processed_value = "unknown artist"
                value = processed_value
            else:
                value = raw_value  # For other columns (including song_name), use the raw value or its generic default
            column_values.append(value)
        df_data[column] = column_values

    # Create the DataFrame
    df = pd.DataFrame(df_data)

    # Apply artist filter if provided and not empty
    if artist_filter and isinstance(artist_filter, list) and len(artist_filter) > 0:
        if SHOW_VERBOSE_INFO:
            st.info(f"Filtering by artists: {', '.join(artist_filter)}")
        total_points_before_artist_filter = len(df)
        unique_artists_before_artist_filter = df["artist"].nunique()
        df = df[df["artist"].isin(artist_filter)]
        filtered_points_after_artist_filter = len(df)
        unique_artists_after_artist_filter = df["artist"].nunique()

        if SHOW_VERBOSE_INFO:
            st.info(
                f"Filtered from {total_points_before_artist_filter} points to {filtered_points_after_artist_filter} points based on artist selection."
            )
        if SHOW_VERBOSE_INFO:
            st.info(
                f"Filtered from {unique_artists_before_artist_filter} artists to {unique_artists_after_artist_filter} artists based on artist selection."
            )
        if df.empty:
            st.warning(
                "No data points remaining after artist filtering. Please adjust your selection."
            )
            return None

    # Filter by song_name if song_filter is provided
    if song_filter and isinstance(song_filter, list) and len(song_filter) > 0:
        total_points_before_song_filter = len(df)
        unique_songs_before_song_filter = df["song_name"].nunique()
        if SHOW_VERBOSE_INFO:
            st.info(f"Applying song filter: {song_filter}")
        df = df[df["song_name"].isin(song_filter)]
        filtered_points_after_song_filter = len(df)
        unique_songs_after_song_filter = df["song_name"].nunique()

        if SHOW_VERBOSE_INFO:
            st.info(
                f"Filtered from {total_points_before_song_filter} points to {filtered_points_after_song_filter} points based on song selection."
            )
        if SHOW_VERBOSE_INFO:
            st.info(
                f"Filtered from {unique_songs_before_song_filter} songs to {unique_songs_after_song_filter} songs based on song selection."
            )
        if df.empty:
            st.warning(
                "No data points remaining after song filtering. Please adjust your selection."
            )
            return None

    # Filter by maqam if maqam_filter is provided
    if maqam_filter and isinstance(maqam_filter, list) and len(maqam_filter) > 0:
        total_points_before_maqam_filter = len(df)
        unique_maqams_before_maqam_filter = df["maqam"].nunique()
        if SHOW_VERBOSE_INFO:
            st.info(f"Applying maqam filter: {maqam_filter}")
        df = df[df["maqam"].isin(maqam_filter)]
        filtered_points_after_maqam_filter = len(df)
        unique_maqams_after_maqam_filter = df["maqam"].nunique()

        if SHOW_VERBOSE_INFO:
            st.info(
                f"Filtered from {total_points_before_maqam_filter} points to {filtered_points_after_maqam_filter} points based on maqam selection."
            )
        if SHOW_VERBOSE_INFO:
            st.info(
                f"Filtered from {unique_maqams_before_maqam_filter} maqams to {unique_maqams_after_maqam_filter} maqams based on maqam selection."
            )
        if df.empty:
            st.warning(
                "No data points remaining after maqam filtering. Please adjust your selection."
            )
            return None

    # Sort the DataFrame by video_id and chunk_number for the line paths
    df = df.sort_values(["video_id", "chunk_number"])

    # Base chart
    base = alt.Chart(df).encode(
        x=alt.X(
            "x:Q",
            title=None,
            axis=alt.Axis(labels=False, ticks=False, domain=False, grid=True),
        ),
        y=alt.Y(
            "y:Q",
            title=None,
            axis=alt.Axis(labels=False, ticks=False, domain=False, grid=True),
        ),
    )

    # Create lines between consecutive chunks
    lines_data = []
    if show_lines:
        for vid in df["video_id"].unique():
            vid_df = df[df["video_id"] == vid].sort_values("chunk_number")
            if len(vid_df) > 1:
                for i in range(len(vid_df) - 1):
                    line_data = {
                        "x1": vid_df.iloc[i]["x"],
                        "y1": vid_df.iloc[i]["y"],
                        "x2": vid_df.iloc[i + 1]["x"],
                        "y2": vid_df.iloc[i + 1]["y"],
                    }
                    # Add all metadata columns
                    for col in [
                        "song_name",
                        "artist",
                        "maqam",
                        "type",
                        "electric",
                        "vintage",
                        "link",
                    ]:
                        line_data[col] = vid_df.iloc[i][col]
                    lines_data.append(line_data)

    # Create a DataFrame for the lines
    lines_df = pd.DataFrame(lines_data) if lines_data else pd.DataFrame()

    # Create the lines layer if we have line data

    # Create a selection that chooses the legend item for interactive opacity
    legend_selection = alt.selection_multi(fields=[color_selection], bind="legend")
    lines = None
    if not lines_df.empty and show_lines:
        lines = (
            alt.Chart(lines_df)
            .mark_line(strokeWidth=1.5)  # Original strokeWidth, dynamic opacity
            .encode(
                x="x1:Q",
                y="y1:Q",
                x2="x2:Q",
                y2="y2:Q",
                color=alt.Color(
                    f"{color_selection}:N",
                    legend=None,  # Hide legend for lines
                ),
                opacity=alt.condition(
                    legend_selection, alt.value(0.7), alt.value(0.1)
                ),  # Conditional opacity for lines
            )
            .add_selection(legend_selection)
        )

    # Create the points layer
    points = (
        base.mark_circle(size=100)
        .encode(
            color=alt.Color(
                f"{color_selection}:N",
                legend=alt.Legend(
                    title=color_selection.replace("_", " ").title(),
                ),
            ),
            tooltip=[
                "video_id:N",  # For UUID
                "song_name:N",
                "artist:N",
                "maqam:N",
                "type:N",  # Restored
                "electric:N",  # Restored
                "vintage:N",
                "chunk_number:Q",
                "link:N",
            ],
            opacity=alt.condition(legend_selection, alt.value(1.0), alt.value(0.05)),
        )
        .add_selection(legend_selection)
    )

    # Create the text layer for chunk numbers if requested
    text = None
    if show_chunk_numbers:
        text = (
            base.mark_text(align="center", baseline="middle", dy=-10, fontSize=10)
            .encode(
                text="chunk_number:N",
                color=alt.value("black"),
                opacity=alt.condition(legend_selection, alt.value(1.0), alt.value(0.2)),
            )
            .add_selection(legend_selection)
        )

    # Combine the layers
    layers = [points]
    if lines is not None:
        layers.append(lines)
    if text is not None:
        layers.append(text)

    # Create the final chart
    chart = (
        alt.layer(*layers)
        .properties(
            width=900,
            height=1000,
            title=f"MAEST Audio Embeddings - {embedding_type.upper()} Tokens ({reduction_method.upper()})",
            padding={"left": 0, "top": 0, "right": 0, "bottom": 0},
        )
        .resolve_legend(color="independent")
        .configure_legend(
            orient="right",
            labelLimit=75,
            symbolLimit=0,
            columns=1,
        )
        .interactive()
    )

    return chart


# Callback for multiselect artist filter
def callback_multiselect_artists():
    # This callback is for the multiselect widget
    # It reads the multiselect's current selection (which triggered the callback)
    # and updates the main selected_artists_for_filter list.
    st.session_state.selected_artists_for_filter = st.session_state.get(
        "multiselect_artist_value", []
    )[:]


# Callback for multiselect song filter
def callback_multiselect_songs():
    st.session_state.selected_songs_for_filter = st.session_state.get(
        "multiselect_song_value", []
    )[:]


# Callback for multiselect maqam filter
def callback_multiselect_maqams():
    st.session_state.selected_maqams_for_filter = st.session_state.get(
        "multiselect_maqam_value", []
    )[:]


def embedding_visualizer_ui():
    """
    Streamlit UI for the embedding visualizer
    """
    global SHOW_VERBOSE_INFO

    # Initialize embedding normalization choice in session state
    if "embedding_normalization" not in st.session_state:
        st.session_state.embedding_normalization = "Original"

    st.subheader("Embeddings Visualization")
    st.write(
        "This page will provide interactive visualizations of audio embeddings."
        "Each dot represent a 30 second segments of the taqsim, with the numbering representing"
        " the segment order."
    )

    # Initialize session state for the verbose info checkbox if it's not already set.
    # This ensures that on the first run of a new session, it takes the global default (False).
    # The global SHOW_VERBOSE_INFO (defined at the top of the script) is used as this default.
    if "show_verbose_info_checkbox_val" not in st.session_state:
        st.session_state.show_verbose_info_checkbox_val = (
            SHOW_VERBOSE_INFO  # Uses the global SHOW_VERBOSE_INFO
        )

    # Synchronize the function-scoped SHOW_VERBOSE_INFO with the session state.
    # On a fresh session, this will be the default. On refreshes, it will be the persisted value.
    SHOW_VERBOSE_INFO = st.session_state.show_verbose_info_checkbox_val

    # Sidebar for configuration
    st.sidebar.markdown("## Visualization Settings")

    # Select embeddings directory based on session state
    if st.session_state.embedding_normalization == "Original":
        embeddings_dir = EMBEDDINGS_DIR_PATH
    else:  # "Artist-Normalized"
        embeddings_dir = NORMALIZED_EMBEDDINGS_DIR_PATH

    if SHOW_VERBOSE_INFO:
        st.info(f"Using embeddings from: {embeddings_dir}")

    # Embedding type selection
    embedding_type = st.sidebar.selectbox(
        "Embedding Type",
        ["cls", "dist", "avg", "combined"],
        index=3,
    )

    # Dimensionality reduction method selection
    reduction_method = st.sidebar.radio(
        "Dimensionality Reduction Method",
        ["UMAP", "T-SNE", "PCA"],
        index=0,  # Default to UMAP
    )

    # UMAP parameters section (only shown when UMAP is selected)
    if reduction_method == "UMAP":
        st.sidebar.subheader("UMAP Parameters")

        # Initialize n_neighbors in session state if not already set
        if "umap_n_neighbors" not in st.session_state:
            st.session_state.umap_n_neighbors = 15

        # Slider for n_neighbors parameter
        st.sidebar.slider(
            "Number of Neighbors",
            min_value=2,
            max_value=100,
            value=st.session_state.umap_n_neighbors,
            step=1,
            help="Controls the balance between local and global structure. Lower values (2-15) preserve local structure, higher values (30-100) preserve global structure.",
            key="umap_n_neighbors",
        )

    # Artist Filter Section
    current_artist_options = get_all_artists_from_csv()
    if "multiselect_artist_value" not in st.session_state:
        st.session_state.multiselect_artist_value = current_artist_options[:]
    else:
        st.session_state.multiselect_artist_value = [
            artist
            for artist in st.session_state.multiselect_artist_value
            if artist in current_artist_options
        ]
        if not current_artist_options:
            st.session_state.multiselect_artist_value = []
    if "selected_artists_for_filter" not in st.session_state:
        st.session_state.selected_artists_for_filter = (
            st.session_state.multiselect_artist_value[:]
        )
    if not current_artist_options:
        st.sidebar.warning("No artists found in metadata to filter by.")
        artist_filter_for_visualization = []
    else:
        with st.sidebar.expander("Filter by Artist", expanded=False):
            st.multiselect(
                "",
                options=current_artist_options,
                key="multiselect_artist_value",
                on_change=callback_multiselect_artists,
            )
    artist_filter_for_visualization = st.session_state.get(
        "selected_artists_for_filter", []
    )

    # --- Song Filter Section ---
    current_song_options = get_all_songs_from_csv()
    if "multiselect_song_value" not in st.session_state:
        st.session_state.multiselect_song_value = current_song_options[:]
    else:
        st.session_state.multiselect_song_value = [
            song
            for song in st.session_state.multiselect_song_value
            if song in current_song_options
        ]
        if not current_song_options:
            st.session_state.multiselect_song_value = []
    if "selected_songs_for_filter" not in st.session_state:
        st.session_state.selected_songs_for_filter = (
            st.session_state.multiselect_song_value[:]
        )
    st.session_state.selected_songs_for_filter = (
        st.session_state.multiselect_song_value[:]
    )
    if not current_song_options:
        st.sidebar.warning("No songs found in metadata to filter by.")
        song_filter_for_visualization = []
    else:
        with st.sidebar.expander("Filter by Taqsim", expanded=False):
            st.multiselect(
                "",
                options=current_song_options,
                key="multiselect_song_value",
                on_change=callback_multiselect_songs,
            )
    song_filter_for_visualization = st.session_state.get(
        "selected_songs_for_filter", []
    )

    # --- Maqam Filter Section ---
    current_maqam_options = get_all_maqams_from_csv()
    if "multiselect_maqam_value" not in st.session_state:
        st.session_state.multiselect_maqam_value = current_maqam_options[:]
    else:
        st.session_state.multiselect_maqam_value = [
            maqam
            for maqam in st.session_state.multiselect_maqam_value
            if maqam in current_maqam_options
        ]
        if not current_maqam_options:
            st.session_state.multiselect_maqam_value = []
    if "selected_maqams_for_filter" not in st.session_state:
        st.session_state.selected_maqams_for_filter = (
            st.session_state.multiselect_maqam_value[:]
        )
    st.session_state.selected_maqams_for_filter = (
        st.session_state.multiselect_maqam_value[:]
    )
    if not current_maqam_options:
        st.sidebar.warning("No maqams found in metadata to filter by.")
        maqam_filter_for_visualization = []
    else:
        with st.sidebar.expander("Filter by Maqam", expanded=False):
            st.multiselect(
                "",
                options=current_maqam_options,
                key="multiselect_maqam_value",
                on_change=callback_multiselect_maqams,
            )
    maqam_filter_for_visualization = st.session_state.get(
        "selected_maqams_for_filter", []
    )

    # Normalization options
    st.sidebar.subheader("Normalization")

    # Radio button for selecting embedding source
    st.sidebar.radio(
        "Embedding Source",
        options=["Original", "Artist-Normalized"],
        key="embedding_normalization",
    )

    # Chunk filtering options
    st.sidebar.subheader("Chunk Filtering")
    exclude_last_chunk = st.sidebar.checkbox(
        "Exclude Last Segment",
        value=True,
    )
    only_first_chunk = st.sidebar.checkbox(
        "Only First Segment",
        value=False,
    )

    st.sidebar.markdown("---")  # Separator
    st.sidebar.subheader("Chart Appearance")

    color_selection = st.sidebar.selectbox(
        "Color by:",
        ["song_name", "artist", "maqam", "type", "electric", "vintage"],
        index=0,
    )
    show_lines = st.sidebar.checkbox("Show connecting lines", value=True)
    show_chunk_numbers = st.sidebar.checkbox("Show Segment Numbers", value=True)

    st.sidebar.markdown("---")  # Separator

    # Checkbox to control the global SHOW_VERBOSE_INFO variable.
    # Set its 'value' directly from SHOW_VERBOSE_INFO (which is False at this point on a refresh).
    # The 'key' allows Streamlit to manage its state in st.session_state upon interaction.
    st.sidebar.checkbox(
        "Show Detailed Info Messages",
        key="show_verbose_info_checkbox_val",  # Session state (initialized above or persisted) drives the value
    )

    # After the checkbox widget, update SHOW_VERBOSE_INFO from session state.
    # This ensures that if the user interacted with the checkbox, that state is used
    # for the rest of this script run.
    SHOW_VERBOSE_INFO = st.session_state.show_verbose_info_checkbox_val

    # Create a placeholder for the visualization
    chart_placeholder = st.empty()

    # Automatically generate visualization based on current settings
    if not os.path.exists(embeddings_dir):
        st.error(f"Embeddings directory does not exist: {embeddings_dir}")
    else:
        with st.spinner("Generating visualization..."):
            # Set normalize_by_artist based on the embedding source selection
            normalize_by_artist = False  # We don't need additional normalization since we're using pre-normalized embeddings

            # Get n_neighbors value from session state if UMAP is selected, otherwise use default
            umap_n_neighbors = (
                st.session_state.umap_n_neighbors if reduction_method == "UMAP" else 15
            )

            chart = create_embedding_visualization(
                embeddings_dir=embeddings_dir,
                embedding_type=embedding_type,
                reduction_method=reduction_method,
                exclude_last_chunk=exclude_last_chunk,
                artist_filter=artist_filter_for_visualization,
                song_filter=song_filter_for_visualization,
                maqam_filter=maqam_filter_for_visualization,
                only_first_chunk=only_first_chunk,
                color_selection=color_selection,
                show_lines=show_lines,
                show_chunk_numbers=show_chunk_numbers,
                normalize_by_artist=normalize_by_artist,
                n_neighbors=umap_n_neighbors,
            )

            if chart:
                chart_placeholder.altair_chart(chart, use_container_width=True)


if __name__ == "__main__":
    embedding_visualizer_ui()
